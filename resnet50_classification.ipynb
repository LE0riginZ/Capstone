{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#Configure TensorFlow to use GPU for training\n",
    "tf.config.list_physical_devices('GPU') \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#Set GPU memory limit so my GPU doesn't crash\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create class to record time taken to train the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to record time taken to train per epoch\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "cb = TimingCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create train and test folder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a folder each time the set of code is run to sort dataset into train and test\n",
    "day = date.today()\n",
    "formatDate = day.strftime(\"%b-%d-%Y\")\n",
    "\n",
    "#create folder with current date to group predictions ran in a day together, if it does not exists\n",
    "if os.path.isdir('Prediction (' + formatDate + ')') is False:\n",
    "    folder = os.mkdir('Prediction (' + formatDate + ')')\n",
    "folder = 'Prediction (' + formatDate + ')'\n",
    "\n",
    "#looping through to find if destination folder exists\n",
    "i = 1\n",
    "while True:\n",
    "    if os.path.isdir(folder + '/Battery Images - ' + str(i)) is False:\n",
    "        break\n",
    "    i += 1\n",
    "\n",
    "#destination path for unsorted folder\n",
    "destination = folder + '/Battery Images - ' + str(i)\n",
    "\n",
    "#source path for unsorted folder\n",
    "# source = 'Battery Images/'\n",
    "# source = 'Battery Images [Old]/'\n",
    "# source = 'Battery Images [StreamLined]/'\n",
    "source = 'Battery Images [Download]/'\n",
    "\n",
    "#duplicate the unsorted folder from source to destination\n",
    "shutil.copytree(source, destination)\n",
    "\n",
    "os.mkdir(destination + '/train')\n",
    "os.mkdir(destination + '/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sort dataset into train and test folder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split images into training (80%) and testing (20%) and return both\n",
    "def split(data):\n",
    "    # num = int(len(data) * 0.8)\n",
    "    num = int(len(data) * 0.9) #90/10 split\n",
    "#     num = int(len(data) * 0.7) #70/30 split\n",
    "    random.shuffle(data)\n",
    "    return data[:num], data[num:]\n",
    "\n",
    "dir = os.chdir(destination)\n",
    "for category in os.listdir(dir):\n",
    "    #skip the iteration if folder is \"train\" or \"test\"\n",
    "    skip = ['train', 'test']\n",
    "    if category in skip:\n",
    "        continue\n",
    "    \n",
    "    shutil.move(f'{category}', 'train')\n",
    "    #create a subfolder with the same name in test\n",
    "    os.mkdir(f'test/{category}')\n",
    "\n",
    "    #get the images in each folder (battery type) for spltting of data into training and testing\n",
    "    img = os.listdir(f'train/{category}')\n",
    "    training, testing = split(img)\n",
    "    \n",
    "    for file in testing:\n",
    "        shutil.move(f'train/{category}/{file}', f'test/{category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>a) Simple sequential model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 110, 110, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 108, 108, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 36, 36, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 34, 34, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 32)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,936\n",
      "Trainable params: 48,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(224, 224, 3)))  # 250x250 RGB images\n",
    "model.add(layers.Conv2D(32, 5, strides=2, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(3))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.Conv2D(32, 3, activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "model.add(layers.GlobalMaxPooling2D())\n",
    "# model.add(layers.Dense(5, activation=\"softmax\"))\n",
    "# model.add(layers.Dense(10, activation=\"softmax\")) #For 10 Cats\n",
    "model.add(layers.Dense(8, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>b) Transfer Learning - MobileNet</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"include_top=False argument\" will load a network that excluding the classification layers at the top\n",
    "mobile = tf.keras.applications.mobilenet.MobileNet(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>c) Transfer Learning - MobileNetV2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"include_top=False argument\" will load a network that excluding the classification layers at the top\n",
    "mobile = tf.keras.applications.MobileNetV2(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>c) Transfer Learning - VGG16</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"include_top=False argument\" will load a network that excluding the classification layers at the top\n",
    "mobile = tf.keras.applications.vgg16.VGG16(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>d) Transfer Learning - VGG19</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"include_top=False argument\" will load a network that excluding the classification layers at the top\n",
    "mobile = tf.keras.applications.vgg19.VGG19(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>e) Transfer Learning - ResNet50</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"include_top=False argument\" will load a network that excluding the classification layers at the top\n",
    "mobile = tf.keras.applications.resnet.ResNet50(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>f) Transfer Learning - ResNet50V2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\"include_top=False argument\" will load a network that excluding the classification layers at the top\n",
    "# mobile = tf.keras.applications.ResNet50V2(input_shape = (224, 224, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 16392     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,604,104\n",
      "Trainable params: 16,392\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in mobile.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "x = mobile(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "# x = tf.keras.layers.Dropout(0.65)(x) #Higher dropout to reduce overfitting (last best setting)\n",
    "# output = tf.keras.layers.Dense(5, activation=\"softmax\")(x)   \n",
    "# output = tf.keras.layers.Dense(10, activation=\"softmax\")(x)   #Changed to reflect new categories\n",
    "output = tf.keras.layers.Dense(8, activation=\"softmax\")(x)   #Changed to reflect new categories\n",
    "\n",
    "model = keras.Model(inputs, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Process data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 691 images belonging to 8 classes.\n",
      "Found 73 images belonging to 8 classes.\n",
      "Found 89 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "\n",
    "#for data augmentation on training data\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet.preprocess_input,\n",
    "    # validation_split=0.2,\n",
    "    validation_split=0.1, #changes made for 90/10 split\n",
    "    rotation_range = 10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.2, # Randomly zoom image \n",
    "    width_shift_range = 0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "#     vertical_flip = False,    # randomly flip images\n",
    "    vertical_flip = True, #include vertical flip for better generalisation\n",
    "    )  \n",
    "\n",
    "#takes images from directory path and generates batches of augmented data\n",
    "train = datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    subset='training',\n",
    "        shuffle = False)\n",
    "\n",
    "#for validation data\n",
    "# validation_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input, \n",
    "#                                         validation_split=0.2)\n",
    "validation_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input, \n",
    "                                        validation_split=0.1) #changes made for 90/10 split\n",
    "\n",
    "\n",
    "validation = validation_batches.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    subset='validation',\n",
    "        shuffle = False)\n",
    "\n",
    "#for test data\n",
    "test = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\\\n",
    ".flow_from_directory(directory=test_path, target_size=(224,224), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngden\\anaconda3\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 - 26s - loss: 4.4618 - accuracy: 0.0910 - val_loss: 3.7808 - val_accuracy: 0.1644 - 26s/epoch - 1s/step\n",
      "Epoch 2/300\n",
      "22/22 - 19s - loss: 3.6095 - accuracy: 0.1691 - val_loss: 2.3217 - val_accuracy: 0.2740 - 19s/epoch - 842ms/step\n",
      "Epoch 3/300\n",
      "22/22 - 15s - loss: 3.0056 - accuracy: 0.1951 - val_loss: 2.6906 - val_accuracy: 0.2603 - 15s/epoch - 681ms/step\n",
      "Epoch 4/300\n",
      "22/22 - 16s - loss: 3.2238 - accuracy: 0.1806 - val_loss: 2.3858 - val_accuracy: 0.4658 - 16s/epoch - 736ms/step\n",
      "Epoch 5/300\n",
      "22/22 - 16s - loss: 2.5262 - accuracy: 0.2977 - val_loss: 1.6815 - val_accuracy: 0.4110 - 16s/epoch - 720ms/step\n",
      "Epoch 6/300\n",
      "22/22 - 15s - loss: 2.3788 - accuracy: 0.2659 - val_loss: 1.8689 - val_accuracy: 0.3699 - 15s/epoch - 675ms/step\n",
      "Epoch 7/300\n",
      "22/22 - 15s - loss: 2.3095 - accuracy: 0.2905 - val_loss: 2.3306 - val_accuracy: 0.3425 - 15s/epoch - 686ms/step\n",
      "Epoch 8/300\n",
      "22/22 - 15s - loss: 2.7910 - accuracy: 0.2717 - val_loss: 1.7071 - val_accuracy: 0.4384 - 15s/epoch - 697ms/step\n",
      "Epoch 9/300\n",
      "22/22 - 15s - loss: 2.4374 - accuracy: 0.2688 - val_loss: 1.9985 - val_accuracy: 0.5068 - 15s/epoch - 673ms/step\n",
      "Epoch 10/300\n",
      "22/22 - 14s - loss: 2.2296 - accuracy: 0.3251 - val_loss: 1.7273 - val_accuracy: 0.4110 - 14s/epoch - 638ms/step\n",
      "Epoch 11/300\n",
      "22/22 - 14s - loss: 2.1231 - accuracy: 0.3136 - val_loss: 1.8256 - val_accuracy: 0.4795 - 14s/epoch - 639ms/step\n",
      "Epoch 12/300\n",
      "22/22 - 14s - loss: 2.6611 - accuracy: 0.3266 - val_loss: 1.7598 - val_accuracy: 0.4384 - 14s/epoch - 644ms/step\n",
      "Epoch 13/300\n",
      "22/22 - 14s - loss: 2.3795 - accuracy: 0.3006 - val_loss: 1.9142 - val_accuracy: 0.4384 - 14s/epoch - 640ms/step\n",
      "Epoch 14/300\n",
      "22/22 - 14s - loss: 2.6410 - accuracy: 0.3353 - val_loss: 2.0169 - val_accuracy: 0.5068 - 14s/epoch - 644ms/step\n",
      "Epoch 15/300\n",
      "22/22 - 14s - loss: 2.4679 - accuracy: 0.3454 - val_loss: 1.6410 - val_accuracy: 0.4658 - 14s/epoch - 637ms/step\n",
      "Epoch 16/300\n",
      "22/22 - 14s - loss: 3.2971 - accuracy: 0.2934 - val_loss: 2.7369 - val_accuracy: 0.3836 - 14s/epoch - 639ms/step\n",
      "Epoch 17/300\n",
      "22/22 - 14s - loss: 3.3436 - accuracy: 0.2832 - val_loss: 1.9902 - val_accuracy: 0.4932 - 14s/epoch - 637ms/step\n",
      "Epoch 18/300\n",
      "22/22 - 14s - loss: 2.2258 - accuracy: 0.4176 - val_loss: 1.6559 - val_accuracy: 0.4521 - 14s/epoch - 656ms/step\n",
      "Epoch 19/300\n",
      "22/22 - 15s - loss: 2.0412 - accuracy: 0.3598 - val_loss: 1.6144 - val_accuracy: 0.4795 - 15s/epoch - 667ms/step\n",
      "Epoch 20/300\n",
      "22/22 - 15s - loss: 2.1424 - accuracy: 0.3150 - val_loss: 1.6166 - val_accuracy: 0.4658 - 15s/epoch - 662ms/step\n",
      "Epoch 21/300\n",
      "22/22 - 15s - loss: 1.9918 - accuracy: 0.3945 - val_loss: 1.5016 - val_accuracy: 0.5342 - 15s/epoch - 671ms/step\n",
      "Epoch 22/300\n",
      "22/22 - 14s - loss: 2.4012 - accuracy: 0.2688 - val_loss: 1.5673 - val_accuracy: 0.5205 - 14s/epoch - 654ms/step\n",
      "Epoch 23/300\n",
      "22/22 - 14s - loss: 2.3589 - accuracy: 0.3020 - val_loss: 1.7198 - val_accuracy: 0.4521 - 14s/epoch - 655ms/step\n",
      "Epoch 24/300\n",
      "22/22 - 14s - loss: 2.6953 - accuracy: 0.3483 - val_loss: 1.9397 - val_accuracy: 0.4247 - 14s/epoch - 652ms/step\n",
      "Epoch 25/300\n",
      "22/22 - 14s - loss: 2.0490 - accuracy: 0.3801 - val_loss: 1.6544 - val_accuracy: 0.4932 - 14s/epoch - 649ms/step\n",
      "Epoch 26/300\n",
      "22/22 - 14s - loss: 2.0685 - accuracy: 0.3584 - val_loss: 2.0057 - val_accuracy: 0.4247 - 14s/epoch - 658ms/step\n",
      "Epoch 27/300\n",
      "22/22 - 14s - loss: 1.6180 - accuracy: 0.4610 - val_loss: 1.5515 - val_accuracy: 0.4932 - 14s/epoch - 658ms/step\n",
      "Epoch 28/300\n",
      "22/22 - 14s - loss: 1.7073 - accuracy: 0.4436 - val_loss: 1.5358 - val_accuracy: 0.5068 - 14s/epoch - 641ms/step\n",
      "Epoch 29/300\n",
      "22/22 - 15s - loss: 1.8920 - accuracy: 0.3988 - val_loss: 1.7114 - val_accuracy: 0.5616 - 15s/epoch - 663ms/step\n",
      "Epoch 30/300\n",
      "22/22 - 14s - loss: 2.1389 - accuracy: 0.3728 - val_loss: 1.8513 - val_accuracy: 0.4795 - 14s/epoch - 640ms/step\n",
      "Epoch 31/300\n",
      "22/22 - 14s - loss: 2.2419 - accuracy: 0.3468 - val_loss: 1.9688 - val_accuracy: 0.4658 - 14s/epoch - 638ms/step\n",
      "Epoch 32/300\n",
      "22/22 - 14s - loss: 2.1581 - accuracy: 0.3829 - val_loss: 2.0317 - val_accuracy: 0.4521 - 14s/epoch - 637ms/step\n",
      "Epoch 33/300\n",
      "22/22 - 14s - loss: 2.3232 - accuracy: 0.3526 - val_loss: 1.8084 - val_accuracy: 0.5068 - 14s/epoch - 639ms/step\n",
      "Epoch 34/300\n",
      "22/22 - 14s - loss: 2.4702 - accuracy: 0.3454 - val_loss: 2.4792 - val_accuracy: 0.4521 - 14s/epoch - 639ms/step\n",
      "Epoch 35/300\n",
      "22/22 - 14s - loss: 2.7074 - accuracy: 0.3858 - val_loss: 2.1729 - val_accuracy: 0.4110 - 14s/epoch - 642ms/step\n",
      "Epoch 36/300\n",
      "22/22 - 14s - loss: 1.8981 - accuracy: 0.3945 - val_loss: 1.7441 - val_accuracy: 0.5616 - 14s/epoch - 643ms/step\n",
      "Epoch 37/300\n",
      "22/22 - 14s - loss: 1.8867 - accuracy: 0.4321 - val_loss: 1.5212 - val_accuracy: 0.4932 - 14s/epoch - 643ms/step\n",
      "Epoch 38/300\n",
      "22/22 - 15s - loss: 1.9641 - accuracy: 0.3887 - val_loss: 1.4210 - val_accuracy: 0.5342 - 15s/epoch - 680ms/step\n",
      "Epoch 39/300\n",
      "22/22 - 15s - loss: 1.6261 - accuracy: 0.4855 - val_loss: 2.1031 - val_accuracy: 0.4932 - 15s/epoch - 683ms/step\n",
      "Epoch 40/300\n",
      "22/22 - 15s - loss: 2.1398 - accuracy: 0.4118 - val_loss: 2.0225 - val_accuracy: 0.4247 - 15s/epoch - 673ms/step\n",
      "Epoch 41/300\n",
      "22/22 - 15s - loss: 2.2838 - accuracy: 0.3410 - val_loss: 1.4947 - val_accuracy: 0.5205 - 15s/epoch - 693ms/step\n",
      "Epoch 42/300\n",
      "22/22 - 14s - loss: 1.6666 - accuracy: 0.4697 - val_loss: 1.6599 - val_accuracy: 0.4932 - 14s/epoch - 657ms/step\n",
      "Epoch 43/300\n",
      "22/22 - 14s - loss: 2.0218 - accuracy: 0.3931 - val_loss: 2.0543 - val_accuracy: 0.4658 - 14s/epoch - 642ms/step\n",
      "Epoch 44/300\n",
      "22/22 - 14s - loss: 2.1747 - accuracy: 0.3815 - val_loss: 1.8683 - val_accuracy: 0.4932 - 14s/epoch - 643ms/step\n",
      "Epoch 45/300\n",
      "22/22 - 14s - loss: 2.0420 - accuracy: 0.3945 - val_loss: 1.8190 - val_accuracy: 0.4658 - 14s/epoch - 645ms/step\n",
      "Epoch 46/300\n",
      "22/22 - 14s - loss: 1.9890 - accuracy: 0.4249 - val_loss: 1.7961 - val_accuracy: 0.4384 - 14s/epoch - 644ms/step\n",
      "Epoch 47/300\n",
      "22/22 - 14s - loss: 1.5912 - accuracy: 0.4812 - val_loss: 1.5682 - val_accuracy: 0.4932 - 14s/epoch - 642ms/step\n",
      "Epoch 48/300\n",
      "22/22 - 14s - loss: 1.9163 - accuracy: 0.4350 - val_loss: 1.7582 - val_accuracy: 0.4110 - 14s/epoch - 642ms/step\n",
      "Epoch 49/300\n",
      "22/22 - 14s - loss: 1.9954 - accuracy: 0.3902 - val_loss: 1.8208 - val_accuracy: 0.4795 - 14s/epoch - 645ms/step\n",
      "Epoch 50/300\n",
      "22/22 - 14s - loss: 2.1252 - accuracy: 0.3540 - val_loss: 1.6931 - val_accuracy: 0.5342 - 14s/epoch - 645ms/step\n",
      "Epoch 51/300\n",
      "22/22 - 14s - loss: 1.9332 - accuracy: 0.4740 - val_loss: 1.9202 - val_accuracy: 0.5479 - 14s/epoch - 644ms/step\n",
      "Epoch 52/300\n",
      "22/22 - 14s - loss: 1.6677 - accuracy: 0.4957 - val_loss: 1.9930 - val_accuracy: 0.4795 - 14s/epoch - 647ms/step\n",
      "Epoch 53/300\n",
      "22/22 - 14s - loss: 1.7909 - accuracy: 0.4624 - val_loss: 1.8934 - val_accuracy: 0.5205 - 14s/epoch - 643ms/step\n",
      "Epoch 54/300\n",
      "22/22 - 14s - loss: 1.8108 - accuracy: 0.4335 - val_loss: 1.5073 - val_accuracy: 0.5205 - 14s/epoch - 646ms/step\n",
      "Epoch 55/300\n",
      "22/22 - 14s - loss: 1.7874 - accuracy: 0.4379 - val_loss: 1.7033 - val_accuracy: 0.5068 - 14s/epoch - 645ms/step\n",
      "Epoch 56/300\n",
      "22/22 - 14s - loss: 1.9204 - accuracy: 0.4147 - val_loss: 1.6028 - val_accuracy: 0.5342 - 14s/epoch - 650ms/step\n",
      "Epoch 57/300\n",
      "22/22 - 14s - loss: 1.9302 - accuracy: 0.4133 - val_loss: 1.6833 - val_accuracy: 0.5068 - 14s/epoch - 656ms/step\n",
      "Epoch 58/300\n",
      "22/22 - 14s - loss: 1.9238 - accuracy: 0.4480 - val_loss: 2.2787 - val_accuracy: 0.4110 - 14s/epoch - 647ms/step\n",
      "Epoch 59/300\n",
      "22/22 - 14s - loss: 2.0953 - accuracy: 0.3642 - val_loss: 1.6841 - val_accuracy: 0.4384 - 14s/epoch - 644ms/step\n",
      "Epoch 60/300\n",
      "22/22 - 14s - loss: 1.6725 - accuracy: 0.4668 - val_loss: 1.6952 - val_accuracy: 0.5342 - 14s/epoch - 645ms/step\n",
      "Epoch 61/300\n",
      "22/22 - 14s - loss: 2.0379 - accuracy: 0.4422 - val_loss: 1.3821 - val_accuracy: 0.5479 - 14s/epoch - 641ms/step\n",
      "Epoch 62/300\n",
      "22/22 - 14s - loss: 2.3042 - accuracy: 0.3251 - val_loss: 2.1016 - val_accuracy: 0.5068 - 14s/epoch - 655ms/step\n",
      "Epoch 63/300\n",
      "22/22 - 14s - loss: 2.2096 - accuracy: 0.3757 - val_loss: 1.6761 - val_accuracy: 0.5342 - 14s/epoch - 649ms/step\n",
      "Epoch 64/300\n",
      "22/22 - 14s - loss: 2.0677 - accuracy: 0.4335 - val_loss: 1.4761 - val_accuracy: 0.5205 - 14s/epoch - 640ms/step\n",
      "Epoch 65/300\n",
      "22/22 - 14s - loss: 1.7505 - accuracy: 0.4566 - val_loss: 1.6063 - val_accuracy: 0.5205 - 14s/epoch - 640ms/step\n",
      "Epoch 66/300\n",
      "22/22 - 14s - loss: 1.9062 - accuracy: 0.3916 - val_loss: 1.6381 - val_accuracy: 0.5342 - 14s/epoch - 639ms/step\n",
      "Epoch 67/300\n",
      "22/22 - 15s - loss: 1.9788 - accuracy: 0.4321 - val_loss: 1.3895 - val_accuracy: 0.6164 - 15s/epoch - 681ms/step\n",
      "Epoch 68/300\n",
      "22/22 - 14s - loss: 1.5633 - accuracy: 0.4855 - val_loss: 1.6964 - val_accuracy: 0.4384 - 14s/epoch - 641ms/step\n",
      "Epoch 69/300\n",
      "22/22 - 14s - loss: 1.7062 - accuracy: 0.4639 - val_loss: 1.8062 - val_accuracy: 0.5342 - 14s/epoch - 645ms/step\n",
      "Epoch 70/300\n",
      "22/22 - 14s - loss: 1.6952 - accuracy: 0.4884 - val_loss: 1.4270 - val_accuracy: 0.5205 - 14s/epoch - 641ms/step\n",
      "Epoch 71/300\n",
      "22/22 - 14s - loss: 1.6203 - accuracy: 0.4899 - val_loss: 2.0938 - val_accuracy: 0.4795 - 14s/epoch - 641ms/step\n",
      "Epoch 72/300\n",
      "22/22 - 14s - loss: 1.9550 - accuracy: 0.4422 - val_loss: 1.6586 - val_accuracy: 0.4795 - 14s/epoch - 645ms/step\n",
      "Epoch 73/300\n",
      "22/22 - 14s - loss: 1.6942 - accuracy: 0.4986 - val_loss: 2.3382 - val_accuracy: 0.4247 - 14s/epoch - 641ms/step\n",
      "Epoch 74/300\n",
      "22/22 - 14s - loss: 2.0132 - accuracy: 0.4046 - val_loss: 1.9140 - val_accuracy: 0.5342 - 14s/epoch - 641ms/step\n",
      "Epoch 75/300\n",
      "22/22 - 14s - loss: 1.9921 - accuracy: 0.4118 - val_loss: 2.1933 - val_accuracy: 0.4795 - 14s/epoch - 640ms/step\n",
      "Epoch 76/300\n",
      "22/22 - 14s - loss: 1.9510 - accuracy: 0.4725 - val_loss: 1.8148 - val_accuracy: 0.4932 - 14s/epoch - 643ms/step\n",
      "Epoch 77/300\n",
      "22/22 - 14s - loss: 1.6392 - accuracy: 0.4682 - val_loss: 2.0754 - val_accuracy: 0.4110 - 14s/epoch - 641ms/step\n",
      "Epoch 78/300\n",
      "22/22 - 14s - loss: 2.2288 - accuracy: 0.4494 - val_loss: 2.4822 - val_accuracy: 0.4658 - 14s/epoch - 641ms/step\n",
      "Epoch 79/300\n",
      "22/22 - 14s - loss: 2.1674 - accuracy: 0.4335 - val_loss: 1.8241 - val_accuracy: 0.4795 - 14s/epoch - 643ms/step\n",
      "Epoch 80/300\n",
      "22/22 - 14s - loss: 1.9890 - accuracy: 0.4292 - val_loss: 1.9903 - val_accuracy: 0.4521 - 14s/epoch - 640ms/step\n",
      "Epoch 81/300\n",
      "22/22 - 14s - loss: 1.8239 - accuracy: 0.4624 - val_loss: 1.9873 - val_accuracy: 0.5342 - 14s/epoch - 642ms/step\n",
      "Epoch 82/300\n",
      "22/22 - 14s - loss: 1.9310 - accuracy: 0.4552 - val_loss: 1.8124 - val_accuracy: 0.3973 - 14s/epoch - 645ms/step\n",
      "Epoch 83/300\n",
      "22/22 - 14s - loss: 1.7973 - accuracy: 0.4451 - val_loss: 1.6197 - val_accuracy: 0.5616 - 14s/epoch - 646ms/step\n",
      "Epoch 84/300\n",
      "22/22 - 14s - loss: 2.1986 - accuracy: 0.4191 - val_loss: 2.1034 - val_accuracy: 0.4658 - 14s/epoch - 643ms/step\n",
      "Epoch 85/300\n",
      "22/22 - 14s - loss: 2.0534 - accuracy: 0.4480 - val_loss: 1.5880 - val_accuracy: 0.5753 - 14s/epoch - 646ms/step\n",
      "Epoch 86/300\n",
      "22/22 - 14s - loss: 1.6026 - accuracy: 0.5116 - val_loss: 2.2490 - val_accuracy: 0.5616 - 14s/epoch - 642ms/step\n",
      "Epoch 87/300\n",
      "22/22 - 14s - loss: 1.7314 - accuracy: 0.5332 - val_loss: 2.0376 - val_accuracy: 0.4110 - 14s/epoch - 641ms/step\n",
      "Epoch 88/300\n",
      "22/22 - 14s - loss: 1.8161 - accuracy: 0.4335 - val_loss: 1.5035 - val_accuracy: 0.5205 - 14s/epoch - 646ms/step\n",
      "Epoch 89/300\n",
      "22/22 - 14s - loss: 1.8876 - accuracy: 0.4379 - val_loss: 2.2828 - val_accuracy: 0.4795 - 14s/epoch - 641ms/step\n",
      "Epoch 90/300\n",
      "22/22 - 14s - loss: 1.8266 - accuracy: 0.4899 - val_loss: 1.7431 - val_accuracy: 0.5342 - 14s/epoch - 641ms/step\n",
      "Epoch 91/300\n",
      "22/22 - 14s - loss: 2.1492 - accuracy: 0.3988 - val_loss: 2.1621 - val_accuracy: 0.5068 - 14s/epoch - 641ms/step\n",
      "Epoch 92/300\n",
      "22/22 - 14s - loss: 2.2243 - accuracy: 0.4408 - val_loss: 2.2598 - val_accuracy: 0.5068 - 14s/epoch - 646ms/step\n",
      "Epoch 93/300\n",
      "22/22 - 14s - loss: 1.7658 - accuracy: 0.5043 - val_loss: 2.0222 - val_accuracy: 0.3562 - 14s/epoch - 642ms/step\n",
      "Epoch 94/300\n",
      "22/22 - 14s - loss: 1.7364 - accuracy: 0.4971 - val_loss: 1.8752 - val_accuracy: 0.4795 - 14s/epoch - 648ms/step\n",
      "Epoch 95/300\n",
      "22/22 - 14s - loss: 2.1526 - accuracy: 0.3801 - val_loss: 2.0483 - val_accuracy: 0.5616 - 14s/epoch - 644ms/step\n",
      "Epoch 96/300\n",
      "22/22 - 14s - loss: 2.2137 - accuracy: 0.4494 - val_loss: 2.0422 - val_accuracy: 0.5068 - 14s/epoch - 639ms/step\n",
      "Epoch 97/300\n",
      "22/22 - 14s - loss: 2.0251 - accuracy: 0.4523 - val_loss: 2.1373 - val_accuracy: 0.4658 - 14s/epoch - 640ms/step\n",
      "Epoch 98/300\n",
      "22/22 - 14s - loss: 1.3378 - accuracy: 0.5549 - val_loss: 1.9123 - val_accuracy: 0.4795 - 14s/epoch - 640ms/step\n",
      "Epoch 99/300\n",
      "22/22 - 14s - loss: 1.6217 - accuracy: 0.5116 - val_loss: 2.0337 - val_accuracy: 0.4658 - 14s/epoch - 641ms/step\n",
      "Epoch 100/300\n",
      "22/22 - 14s - loss: 1.7002 - accuracy: 0.4971 - val_loss: 1.9729 - val_accuracy: 0.4795 - 14s/epoch - 639ms/step\n",
      "Epoch 101/300\n",
      "22/22 - 14s - loss: 1.9029 - accuracy: 0.4393 - val_loss: 2.2742 - val_accuracy: 0.5068 - 14s/epoch - 640ms/step\n",
      "Epoch 102/300\n",
      "22/22 - 14s - loss: 1.7303 - accuracy: 0.4841 - val_loss: 1.6848 - val_accuracy: 0.5068 - 14s/epoch - 642ms/step\n",
      "Epoch 103/300\n",
      "22/22 - 14s - loss: 1.6207 - accuracy: 0.4928 - val_loss: 1.6115 - val_accuracy: 0.5205 - 14s/epoch - 640ms/step\n",
      "Epoch 104/300\n",
      "22/22 - 14s - loss: 1.6152 - accuracy: 0.4841 - val_loss: 1.7372 - val_accuracy: 0.5890 - 14s/epoch - 640ms/step\n",
      "Epoch 105/300\n",
      "22/22 - 14s - loss: 1.7028 - accuracy: 0.4566 - val_loss: 1.5751 - val_accuracy: 0.5479 - 14s/epoch - 643ms/step\n",
      "Epoch 106/300\n",
      "22/22 - 14s - loss: 1.7899 - accuracy: 0.4668 - val_loss: 1.8374 - val_accuracy: 0.4932 - 14s/epoch - 641ms/step\n",
      "Epoch 107/300\n",
      "22/22 - 14s - loss: 1.6730 - accuracy: 0.5000 - val_loss: 2.0935 - val_accuracy: 0.4932 - 14s/epoch - 641ms/step\n",
      "Epoch 108/300\n",
      "22/22 - 14s - loss: 2.4672 - accuracy: 0.4075 - val_loss: 1.8254 - val_accuracy: 0.5342 - 14s/epoch - 640ms/step\n",
      "Epoch 109/300\n",
      "22/22 - 14s - loss: 2.3804 - accuracy: 0.3960 - val_loss: 2.4416 - val_accuracy: 0.5342 - 14s/epoch - 641ms/step\n",
      "Epoch 110/300\n",
      "22/22 - 14s - loss: 2.0224 - accuracy: 0.4451 - val_loss: 2.0973 - val_accuracy: 0.4521 - 14s/epoch - 640ms/step\n",
      "Epoch 111/300\n",
      "22/22 - 14s - loss: 2.0900 - accuracy: 0.4422 - val_loss: 2.0696 - val_accuracy: 0.3699 - 14s/epoch - 641ms/step\n",
      "Epoch 112/300\n",
      "22/22 - 14s - loss: 1.5200 - accuracy: 0.5260 - val_loss: 1.7814 - val_accuracy: 0.5205 - 14s/epoch - 641ms/step\n",
      "Epoch 113/300\n",
      "22/22 - 14s - loss: 1.7898 - accuracy: 0.4957 - val_loss: 2.1200 - val_accuracy: 0.4795 - 14s/epoch - 638ms/step\n",
      "Epoch 114/300\n",
      "22/22 - 14s - loss: 2.1134 - accuracy: 0.4451 - val_loss: 2.3685 - val_accuracy: 0.4658 - 14s/epoch - 642ms/step\n",
      "Epoch 115/300\n",
      "22/22 - 14s - loss: 2.0432 - accuracy: 0.4350 - val_loss: 1.8574 - val_accuracy: 0.5068 - 14s/epoch - 639ms/step\n",
      "Epoch 116/300\n",
      "22/22 - 14s - loss: 1.6581 - accuracy: 0.4769 - val_loss: 2.1189 - val_accuracy: 0.5205 - 14s/epoch - 639ms/step\n",
      "Epoch 117/300\n",
      "22/22 - 14s - loss: 2.2406 - accuracy: 0.4176 - val_loss: 2.4691 - val_accuracy: 0.4384 - 14s/epoch - 640ms/step\n",
      "Epoch 118/300\n",
      "22/22 - 14s - loss: 2.1830 - accuracy: 0.4855 - val_loss: 1.9541 - val_accuracy: 0.5205 - 14s/epoch - 641ms/step\n",
      "Epoch 119/300\n",
      "22/22 - 14s - loss: 1.8895 - accuracy: 0.4321 - val_loss: 1.8641 - val_accuracy: 0.5205 - 14s/epoch - 642ms/step\n",
      "Epoch 120/300\n",
      "22/22 - 14s - loss: 1.6042 - accuracy: 0.4899 - val_loss: 1.7970 - val_accuracy: 0.5342 - 14s/epoch - 640ms/step\n",
      "Epoch 121/300\n",
      "22/22 - 15s - loss: 1.4309 - accuracy: 0.5101 - val_loss: 2.0475 - val_accuracy: 0.4795 - 15s/epoch - 660ms/step\n",
      "Epoch 122/300\n",
      "22/22 - 14s - loss: 1.9826 - accuracy: 0.4364 - val_loss: 2.0953 - val_accuracy: 0.4658 - 14s/epoch - 647ms/step\n",
      "Epoch 123/300\n",
      "22/22 - 14s - loss: 1.8440 - accuracy: 0.4870 - val_loss: 2.1028 - val_accuracy: 0.4384 - 14s/epoch - 641ms/step\n",
      "Epoch 124/300\n",
      "22/22 - 14s - loss: 1.9368 - accuracy: 0.5043 - val_loss: 2.3082 - val_accuracy: 0.4384 - 14s/epoch - 641ms/step\n",
      "Epoch 125/300\n",
      "22/22 - 14s - loss: 2.2868 - accuracy: 0.3699 - val_loss: 1.9167 - val_accuracy: 0.6027 - 14s/epoch - 644ms/step\n",
      "Epoch 126/300\n",
      "22/22 - 14s - loss: 2.1073 - accuracy: 0.4624 - val_loss: 1.9251 - val_accuracy: 0.5205 - 14s/epoch - 643ms/step\n",
      "Epoch 127/300\n",
      "22/22 - 14s - loss: 2.0073 - accuracy: 0.4711 - val_loss: 2.3199 - val_accuracy: 0.3699 - 14s/epoch - 642ms/step\n",
      "Epoch 128/300\n",
      "22/22 - 14s - loss: 1.5869 - accuracy: 0.5188 - val_loss: 2.1240 - val_accuracy: 0.4932 - 14s/epoch - 641ms/step\n",
      "Epoch 129/300\n",
      "22/22 - 14s - loss: 1.6710 - accuracy: 0.5087 - val_loss: 1.8864 - val_accuracy: 0.5068 - 14s/epoch - 641ms/step\n",
      "Epoch 130/300\n",
      "22/22 - 14s - loss: 2.1578 - accuracy: 0.4046 - val_loss: 2.0342 - val_accuracy: 0.4521 - 14s/epoch - 641ms/step\n",
      "Epoch 131/300\n",
      "22/22 - 14s - loss: 2.0882 - accuracy: 0.4263 - val_loss: 2.3358 - val_accuracy: 0.5068 - 14s/epoch - 641ms/step\n",
      "Epoch 132/300\n",
      "22/22 - 14s - loss: 2.1943 - accuracy: 0.4104 - val_loss: 2.0462 - val_accuracy: 0.4795 - 14s/epoch - 639ms/step\n",
      "Epoch 133/300\n",
      "22/22 - 14s - loss: 1.6053 - accuracy: 0.5289 - val_loss: 1.8326 - val_accuracy: 0.5616 - 14s/epoch - 641ms/step\n",
      "Epoch 134/300\n",
      "22/22 - 14s - loss: 1.6407 - accuracy: 0.5289 - val_loss: 1.7654 - val_accuracy: 0.5205 - 14s/epoch - 642ms/step\n",
      "Epoch 135/300\n",
      "22/22 - 14s - loss: 2.0313 - accuracy: 0.4422 - val_loss: 2.1516 - val_accuracy: 0.4932 - 14s/epoch - 641ms/step\n",
      "Epoch 136/300\n",
      "22/22 - 14s - loss: 1.8904 - accuracy: 0.4538 - val_loss: 1.9832 - val_accuracy: 0.5342 - 14s/epoch - 642ms/step\n",
      "Epoch 137/300\n",
      "22/22 - 14s - loss: 1.4010 - accuracy: 0.5318 - val_loss: 1.6901 - val_accuracy: 0.5479 - 14s/epoch - 641ms/step\n",
      "Epoch 138/300\n",
      "22/22 - 14s - loss: 1.6425 - accuracy: 0.4899 - val_loss: 1.9438 - val_accuracy: 0.5068 - 14s/epoch - 640ms/step\n",
      "Epoch 139/300\n",
      "22/22 - 14s - loss: 1.4374 - accuracy: 0.5434 - val_loss: 1.5223 - val_accuracy: 0.6027 - 14s/epoch - 640ms/step\n",
      "Epoch 140/300\n",
      "22/22 - 14s - loss: 1.5265 - accuracy: 0.5159 - val_loss: 2.0389 - val_accuracy: 0.5479 - 14s/epoch - 640ms/step\n",
      "Epoch 141/300\n",
      "22/22 - 14s - loss: 1.6326 - accuracy: 0.5419 - val_loss: 1.9632 - val_accuracy: 0.4247 - 14s/epoch - 641ms/step\n",
      "Epoch 142/300\n",
      "22/22 - 14s - loss: 1.9876 - accuracy: 0.4783 - val_loss: 2.3586 - val_accuracy: 0.5068 - 14s/epoch - 641ms/step\n",
      "Epoch 143/300\n",
      "22/22 - 14s - loss: 2.2943 - accuracy: 0.4581 - val_loss: 2.2954 - val_accuracy: 0.4932 - 14s/epoch - 641ms/step\n",
      "Epoch 144/300\n",
      "22/22 - 14s - loss: 2.1824 - accuracy: 0.4436 - val_loss: 1.9648 - val_accuracy: 0.3973 - 14s/epoch - 641ms/step\n",
      "Epoch 145/300\n",
      "22/22 - 14s - loss: 1.6839 - accuracy: 0.4971 - val_loss: 1.6406 - val_accuracy: 0.6027 - 14s/epoch - 643ms/step\n",
      "Epoch 146/300\n",
      "22/22 - 14s - loss: 1.5083 - accuracy: 0.5159 - val_loss: 1.9232 - val_accuracy: 0.4110 - 14s/epoch - 641ms/step\n",
      "Epoch 147/300\n",
      "22/22 - 14s - loss: 1.8615 - accuracy: 0.4581 - val_loss: 2.4139 - val_accuracy: 0.4795 - 14s/epoch - 641ms/step\n",
      "Epoch 148/300\n",
      "22/22 - 14s - loss: 1.7166 - accuracy: 0.5275 - val_loss: 2.2910 - val_accuracy: 0.5205 - 14s/epoch - 643ms/step\n",
      "Epoch 149/300\n",
      "22/22 - 14s - loss: 1.8968 - accuracy: 0.4682 - val_loss: 2.1457 - val_accuracy: 0.5068 - 14s/epoch - 643ms/step\n",
      "Epoch 150/300\n",
      "22/22 - 14s - loss: 1.4244 - accuracy: 0.5448 - val_loss: 1.9400 - val_accuracy: 0.5753 - 14s/epoch - 643ms/step\n",
      "Epoch 151/300\n",
      "22/22 - 14s - loss: 1.8159 - accuracy: 0.5101 - val_loss: 2.0937 - val_accuracy: 0.4658 - 14s/epoch - 641ms/step\n",
      "Epoch 152/300\n",
      "22/22 - 14s - loss: 1.8991 - accuracy: 0.4697 - val_loss: 2.2468 - val_accuracy: 0.4795 - 14s/epoch - 641ms/step\n",
      "Epoch 153/300\n",
      "22/22 - 14s - loss: 1.5697 - accuracy: 0.5607 - val_loss: 2.0480 - val_accuracy: 0.4932 - 14s/epoch - 639ms/step\n",
      "Epoch 154/300\n",
      "22/22 - 14s - loss: 1.7830 - accuracy: 0.4740 - val_loss: 1.8903 - val_accuracy: 0.4932 - 14s/epoch - 642ms/step\n",
      "Epoch 155/300\n",
      "22/22 - 14s - loss: 1.8088 - accuracy: 0.4884 - val_loss: 2.0614 - val_accuracy: 0.4795 - 14s/epoch - 642ms/step\n",
      "Epoch 156/300\n",
      "22/22 - 14s - loss: 1.8006 - accuracy: 0.4711 - val_loss: 2.0740 - val_accuracy: 0.4658 - 14s/epoch - 642ms/step\n",
      "Epoch 157/300\n",
      "22/22 - 14s - loss: 1.4651 - accuracy: 0.5332 - val_loss: 1.7980 - val_accuracy: 0.5479 - 14s/epoch - 642ms/step\n",
      "Epoch 158/300\n",
      "22/22 - 14s - loss: 1.5766 - accuracy: 0.5231 - val_loss: 2.1605 - val_accuracy: 0.5342 - 14s/epoch - 640ms/step\n",
      "Epoch 159/300\n",
      "22/22 - 14s - loss: 2.0967 - accuracy: 0.4306 - val_loss: 2.0431 - val_accuracy: 0.5205 - 14s/epoch - 643ms/step\n",
      "Epoch 160/300\n",
      "22/22 - 14s - loss: 1.8719 - accuracy: 0.4321 - val_loss: 1.8926 - val_accuracy: 0.5342 - 14s/epoch - 642ms/step\n",
      "Epoch 161/300\n",
      "22/22 - 14s - loss: 1.5177 - accuracy: 0.5694 - val_loss: 2.5137 - val_accuracy: 0.3973 - 14s/epoch - 642ms/step\n",
      "Epoch 162/300\n",
      "22/22 - 14s - loss: 2.0228 - accuracy: 0.4610 - val_loss: 2.0857 - val_accuracy: 0.4795 - 14s/epoch - 642ms/step\n",
      "Epoch 163/300\n",
      "22/22 - 14s - loss: 1.6729 - accuracy: 0.5159 - val_loss: 2.1247 - val_accuracy: 0.5068 - 14s/epoch - 642ms/step\n",
      "Epoch 164/300\n",
      "22/22 - 14s - loss: 2.2151 - accuracy: 0.4292 - val_loss: 2.4269 - val_accuracy: 0.4932 - 14s/epoch - 641ms/step\n",
      "Epoch 165/300\n",
      "22/22 - 14s - loss: 1.7414 - accuracy: 0.4913 - val_loss: 1.9143 - val_accuracy: 0.5342 - 14s/epoch - 642ms/step\n",
      "Epoch 166/300\n",
      "22/22 - 14s - loss: 1.6478 - accuracy: 0.5173 - val_loss: 2.5969 - val_accuracy: 0.4247 - 14s/epoch - 641ms/step\n",
      "Epoch 167/300\n",
      "22/22 - 14s - loss: 1.9833 - accuracy: 0.5173 - val_loss: 2.7116 - val_accuracy: 0.3836 - 14s/epoch - 642ms/step\n",
      "Epoch 168/300\n",
      "22/22 - 14s - loss: 2.1275 - accuracy: 0.4610 - val_loss: 2.4069 - val_accuracy: 0.4521 - 14s/epoch - 643ms/step\n",
      "Epoch 169/300\n",
      "22/22 - 14s - loss: 1.6478 - accuracy: 0.5405 - val_loss: 2.0864 - val_accuracy: 0.4110 - 14s/epoch - 642ms/step\n",
      "Epoch 170/300\n",
      "22/22 - 14s - loss: 1.8118 - accuracy: 0.4899 - val_loss: 2.3131 - val_accuracy: 0.5205 - 14s/epoch - 643ms/step\n",
      "Epoch 171/300\n",
      "22/22 - 15s - loss: 1.7057 - accuracy: 0.4740 - val_loss: 2.1497 - val_accuracy: 0.4795 - 15s/epoch - 669ms/step\n",
      "Epoch 172/300\n",
      "22/22 - 15s - loss: 1.5751 - accuracy: 0.5535 - val_loss: 1.7462 - val_accuracy: 0.5342 - 15s/epoch - 696ms/step\n",
      "Epoch 173/300\n",
      "22/22 - 16s - loss: 1.8465 - accuracy: 0.4581 - val_loss: 2.0382 - val_accuracy: 0.6301 - 16s/epoch - 715ms/step\n",
      "Epoch 174/300\n",
      "22/22 - 15s - loss: 1.7458 - accuracy: 0.5029 - val_loss: 1.8414 - val_accuracy: 0.5205 - 15s/epoch - 679ms/step\n",
      "Epoch 175/300\n",
      "22/22 - 15s - loss: 1.5680 - accuracy: 0.5303 - val_loss: 1.8891 - val_accuracy: 0.4521 - 15s/epoch - 673ms/step\n",
      "Epoch 176/300\n",
      "22/22 - 15s - loss: 1.6784 - accuracy: 0.4827 - val_loss: 1.9718 - val_accuracy: 0.5616 - 15s/epoch - 676ms/step\n",
      "Epoch 177/300\n",
      "22/22 - 15s - loss: 1.6688 - accuracy: 0.5188 - val_loss: 2.1594 - val_accuracy: 0.4658 - 15s/epoch - 667ms/step\n",
      "Epoch 178/300\n",
      "22/22 - 15s - loss: 1.8587 - accuracy: 0.5202 - val_loss: 2.5419 - val_accuracy: 0.4247 - 15s/epoch - 672ms/step\n",
      "Epoch 179/300\n",
      "22/22 - 15s - loss: 1.7608 - accuracy: 0.4711 - val_loss: 2.0284 - val_accuracy: 0.4658 - 15s/epoch - 672ms/step\n",
      "Epoch 180/300\n",
      "22/22 - 15s - loss: 1.8863 - accuracy: 0.4523 - val_loss: 1.6826 - val_accuracy: 0.6301 - 15s/epoch - 670ms/step\n",
      "Epoch 181/300\n",
      "22/22 - 15s - loss: 1.6929 - accuracy: 0.5217 - val_loss: 2.0395 - val_accuracy: 0.5616 - 15s/epoch - 675ms/step\n",
      "Epoch 182/300\n",
      "22/22 - 15s - loss: 1.5714 - accuracy: 0.5159 - val_loss: 1.8514 - val_accuracy: 0.4795 - 15s/epoch - 669ms/step\n",
      "Epoch 183/300\n",
      "22/22 - 15s - loss: 1.7512 - accuracy: 0.4740 - val_loss: 2.0390 - val_accuracy: 0.5342 - 15s/epoch - 673ms/step\n",
      "Epoch 184/300\n",
      "22/22 - 15s - loss: 2.0752 - accuracy: 0.4480 - val_loss: 2.1652 - val_accuracy: 0.4658 - 15s/epoch - 665ms/step\n",
      "Epoch 185/300\n",
      "22/22 - 15s - loss: 1.8158 - accuracy: 0.4812 - val_loss: 1.8573 - val_accuracy: 0.5205 - 15s/epoch - 675ms/step\n",
      "Epoch 186/300\n",
      "22/22 - 15s - loss: 2.5685 - accuracy: 0.3598 - val_loss: 2.1440 - val_accuracy: 0.4795 - 15s/epoch - 667ms/step\n",
      "Epoch 187/300\n",
      "22/22 - 15s - loss: 2.0794 - accuracy: 0.4566 - val_loss: 2.0548 - val_accuracy: 0.4658 - 15s/epoch - 675ms/step\n",
      "Epoch 188/300\n",
      "22/22 - 15s - loss: 1.7449 - accuracy: 0.4957 - val_loss: 1.8592 - val_accuracy: 0.5068 - 15s/epoch - 664ms/step\n",
      "Epoch 189/300\n",
      "22/22 - 15s - loss: 1.4339 - accuracy: 0.5491 - val_loss: 1.8869 - val_accuracy: 0.5205 - 15s/epoch - 677ms/step\n",
      "Epoch 190/300\n",
      "22/22 - 15s - loss: 1.6117 - accuracy: 0.5275 - val_loss: 2.1688 - val_accuracy: 0.4795 - 15s/epoch - 674ms/step\n",
      "Epoch 191/300\n",
      "22/22 - 15s - loss: 1.6066 - accuracy: 0.5361 - val_loss: 2.1637 - val_accuracy: 0.4795 - 15s/epoch - 673ms/step\n",
      "Epoch 192/300\n",
      "22/22 - 15s - loss: 2.0387 - accuracy: 0.4292 - val_loss: 1.9910 - val_accuracy: 0.5068 - 15s/epoch - 673ms/step\n",
      "Epoch 193/300\n",
      "22/22 - 15s - loss: 1.8970 - accuracy: 0.4870 - val_loss: 2.4850 - val_accuracy: 0.5068 - 15s/epoch - 669ms/step\n",
      "Epoch 194/300\n",
      "22/22 - 15s - loss: 2.0876 - accuracy: 0.4754 - val_loss: 1.6931 - val_accuracy: 0.5890 - 15s/epoch - 677ms/step\n",
      "Epoch 195/300\n",
      "22/22 - 15s - loss: 1.6725 - accuracy: 0.5000 - val_loss: 1.9796 - val_accuracy: 0.5205 - 15s/epoch - 670ms/step\n",
      "Epoch 196/300\n",
      "22/22 - 15s - loss: 1.5162 - accuracy: 0.5101 - val_loss: 1.9492 - val_accuracy: 0.4384 - 15s/epoch - 674ms/step\n",
      "Epoch 197/300\n",
      "22/22 - 15s - loss: 2.0164 - accuracy: 0.4566 - val_loss: 2.0604 - val_accuracy: 0.4795 - 15s/epoch - 671ms/step\n",
      "Epoch 198/300\n",
      "22/22 - 15s - loss: 1.7175 - accuracy: 0.5043 - val_loss: 2.1755 - val_accuracy: 0.4658 - 15s/epoch - 678ms/step\n",
      "Epoch 199/300\n",
      "22/22 - 15s - loss: 2.0682 - accuracy: 0.4668 - val_loss: 3.1863 - val_accuracy: 0.4658 - 15s/epoch - 676ms/step\n",
      "Epoch 200/300\n",
      "22/22 - 15s - loss: 2.3195 - accuracy: 0.4523 - val_loss: 2.3237 - val_accuracy: 0.5616 - 15s/epoch - 688ms/step\n",
      "Epoch 201/300\n",
      "22/22 - 15s - loss: 2.0057 - accuracy: 0.4754 - val_loss: 2.1814 - val_accuracy: 0.5342 - 15s/epoch - 681ms/step\n",
      "Epoch 202/300\n",
      "22/22 - 15s - loss: 1.7263 - accuracy: 0.5159 - val_loss: 1.6561 - val_accuracy: 0.5479 - 15s/epoch - 682ms/step\n",
      "Epoch 203/300\n",
      "22/22 - 15s - loss: 1.4943 - accuracy: 0.5347 - val_loss: 1.9046 - val_accuracy: 0.5616 - 15s/epoch - 685ms/step\n",
      "Epoch 204/300\n",
      "22/22 - 15s - loss: 2.0812 - accuracy: 0.4393 - val_loss: 2.2047 - val_accuracy: 0.4795 - 15s/epoch - 674ms/step\n",
      "Epoch 205/300\n",
      "22/22 - 15s - loss: 1.4567 - accuracy: 0.5766 - val_loss: 1.8509 - val_accuracy: 0.5342 - 15s/epoch - 674ms/step\n",
      "Epoch 206/300\n",
      "22/22 - 15s - loss: 1.5756 - accuracy: 0.5405 - val_loss: 2.0925 - val_accuracy: 0.5616 - 15s/epoch - 667ms/step\n",
      "Epoch 207/300\n",
      "22/22 - 15s - loss: 1.5659 - accuracy: 0.5405 - val_loss: 2.2557 - val_accuracy: 0.4384 - 15s/epoch - 675ms/step\n",
      "Epoch 208/300\n",
      "22/22 - 15s - loss: 1.7446 - accuracy: 0.5202 - val_loss: 1.8577 - val_accuracy: 0.4658 - 15s/epoch - 686ms/step\n",
      "Epoch 209/300\n",
      "22/22 - 15s - loss: 1.3614 - accuracy: 0.5621 - val_loss: 2.1860 - val_accuracy: 0.4795 - 15s/epoch - 673ms/step\n",
      "Epoch 210/300\n",
      "22/22 - 15s - loss: 1.3370 - accuracy: 0.5997 - val_loss: 2.2846 - val_accuracy: 0.4658 - 15s/epoch - 675ms/step\n",
      "Epoch 211/300\n",
      "22/22 - 15s - loss: 2.0219 - accuracy: 0.4754 - val_loss: 2.6163 - val_accuracy: 0.4795 - 15s/epoch - 664ms/step\n",
      "Epoch 212/300\n",
      "22/22 - 15s - loss: 1.6407 - accuracy: 0.5882 - val_loss: 2.1375 - val_accuracy: 0.4247 - 15s/epoch - 668ms/step\n",
      "Epoch 213/300\n",
      "22/22 - 15s - loss: 1.7612 - accuracy: 0.4697 - val_loss: 2.2549 - val_accuracy: 0.5205 - 15s/epoch - 690ms/step\n",
      "Epoch 214/300\n",
      "22/22 - 15s - loss: 1.8605 - accuracy: 0.4986 - val_loss: 2.6602 - val_accuracy: 0.4384 - 15s/epoch - 673ms/step\n",
      "Epoch 215/300\n",
      "22/22 - 15s - loss: 1.9639 - accuracy: 0.4465 - val_loss: 1.9788 - val_accuracy: 0.4795 - 15s/epoch - 695ms/step\n",
      "Epoch 216/300\n",
      "22/22 - 15s - loss: 1.5658 - accuracy: 0.5332 - val_loss: 2.2363 - val_accuracy: 0.4521 - 15s/epoch - 680ms/step\n",
      "Epoch 217/300\n",
      "22/22 - 15s - loss: 1.4612 - accuracy: 0.5621 - val_loss: 2.0057 - val_accuracy: 0.4658 - 15s/epoch - 673ms/step\n",
      "Epoch 218/300\n",
      "22/22 - 15s - loss: 1.6987 - accuracy: 0.5231 - val_loss: 2.5227 - val_accuracy: 0.3973 - 15s/epoch - 674ms/step\n",
      "Epoch 219/300\n",
      "22/22 - 15s - loss: 2.0012 - accuracy: 0.4263 - val_loss: 2.2080 - val_accuracy: 0.4384 - 15s/epoch - 669ms/step\n",
      "Epoch 220/300\n",
      "22/22 - 15s - loss: 1.9428 - accuracy: 0.4740 - val_loss: 1.7016 - val_accuracy: 0.5890 - 15s/epoch - 671ms/step\n",
      "Epoch 221/300\n",
      "22/22 - 15s - loss: 1.4832 - accuracy: 0.5535 - val_loss: 1.9159 - val_accuracy: 0.5068 - 15s/epoch - 668ms/step\n",
      "Epoch 222/300\n",
      "22/22 - 15s - loss: 1.6020 - accuracy: 0.5390 - val_loss: 1.7669 - val_accuracy: 0.5616 - 15s/epoch - 678ms/step\n",
      "Epoch 223/300\n",
      "22/22 - 15s - loss: 1.8928 - accuracy: 0.4494 - val_loss: 2.3003 - val_accuracy: 0.5068 - 15s/epoch - 674ms/step\n",
      "Epoch 224/300\n",
      "22/22 - 15s - loss: 1.8030 - accuracy: 0.4884 - val_loss: 2.0535 - val_accuracy: 0.4932 - 15s/epoch - 669ms/step\n",
      "Epoch 225/300\n",
      "22/22 - 15s - loss: 1.4351 - accuracy: 0.5506 - val_loss: 1.8311 - val_accuracy: 0.5205 - 15s/epoch - 678ms/step\n",
      "Epoch 226/300\n",
      "22/22 - 15s - loss: 1.7914 - accuracy: 0.5043 - val_loss: 1.8200 - val_accuracy: 0.5068 - 15s/epoch - 679ms/step\n",
      "Epoch 227/300\n",
      "22/22 - 15s - loss: 2.0179 - accuracy: 0.4480 - val_loss: 2.1267 - val_accuracy: 0.5068 - 15s/epoch - 663ms/step\n",
      "Epoch 228/300\n",
      "22/22 - 15s - loss: 1.9755 - accuracy: 0.5130 - val_loss: 2.1330 - val_accuracy: 0.5342 - 15s/epoch - 673ms/step\n",
      "Epoch 229/300\n",
      "22/22 - 15s - loss: 1.8014 - accuracy: 0.4971 - val_loss: 1.6004 - val_accuracy: 0.5753 - 15s/epoch - 668ms/step\n",
      "Epoch 230/300\n",
      "22/22 - 15s - loss: 1.5766 - accuracy: 0.5434 - val_loss: 1.8167 - val_accuracy: 0.6164 - 15s/epoch - 676ms/step\n",
      "Epoch 231/300\n",
      "22/22 - 15s - loss: 1.6244 - accuracy: 0.5058 - val_loss: 1.9072 - val_accuracy: 0.5616 - 15s/epoch - 676ms/step\n",
      "Epoch 232/300\n",
      "22/22 - 15s - loss: 1.8735 - accuracy: 0.4740 - val_loss: 2.5545 - val_accuracy: 0.4384 - 15s/epoch - 693ms/step\n",
      "Epoch 233/300\n",
      "22/22 - 15s - loss: 2.0467 - accuracy: 0.4971 - val_loss: 2.0808 - val_accuracy: 0.5068 - 15s/epoch - 668ms/step\n",
      "Epoch 234/300\n",
      "22/22 - 15s - loss: 1.7709 - accuracy: 0.4899 - val_loss: 2.2078 - val_accuracy: 0.5342 - 15s/epoch - 668ms/step\n",
      "Epoch 235/300\n",
      "22/22 - 15s - loss: 1.8918 - accuracy: 0.4639 - val_loss: 2.0795 - val_accuracy: 0.4795 - 15s/epoch - 667ms/step\n",
      "Epoch 236/300\n",
      "22/22 - 15s - loss: 1.7034 - accuracy: 0.5130 - val_loss: 1.9721 - val_accuracy: 0.4795 - 15s/epoch - 688ms/step\n",
      "Epoch 237/300\n",
      "22/22 - 16s - loss: 1.9500 - accuracy: 0.4740 - val_loss: 2.3093 - val_accuracy: 0.5479 - 16s/epoch - 711ms/step\n",
      "Epoch 238/300\n",
      "22/22 - 15s - loss: 1.5113 - accuracy: 0.5347 - val_loss: 1.9475 - val_accuracy: 0.5068 - 15s/epoch - 686ms/step\n",
      "Epoch 239/300\n",
      "22/22 - 15s - loss: 1.6923 - accuracy: 0.5000 - val_loss: 1.9655 - val_accuracy: 0.5479 - 15s/epoch - 681ms/step\n",
      "Epoch 240/300\n",
      "22/22 - 15s - loss: 1.8028 - accuracy: 0.5303 - val_loss: 2.4378 - val_accuracy: 0.4795 - 15s/epoch - 681ms/step\n",
      "Epoch 241/300\n",
      "22/22 - 15s - loss: 1.6632 - accuracy: 0.5202 - val_loss: 1.9109 - val_accuracy: 0.5342 - 15s/epoch - 665ms/step\n",
      "Epoch 242/300\n",
      "22/22 - 15s - loss: 1.5889 - accuracy: 0.5318 - val_loss: 2.6061 - val_accuracy: 0.4932 - 15s/epoch - 665ms/step\n",
      "Epoch 243/300\n",
      "22/22 - 15s - loss: 1.7789 - accuracy: 0.5246 - val_loss: 2.3591 - val_accuracy: 0.5753 - 15s/epoch - 664ms/step\n",
      "Epoch 244/300\n",
      "22/22 - 15s - loss: 1.8959 - accuracy: 0.5217 - val_loss: 2.1444 - val_accuracy: 0.4795 - 15s/epoch - 665ms/step\n",
      "Epoch 245/300\n",
      "22/22 - 15s - loss: 1.7726 - accuracy: 0.5159 - val_loss: 2.3541 - val_accuracy: 0.4932 - 15s/epoch - 661ms/step\n",
      "Epoch 246/300\n",
      "22/22 - 15s - loss: 2.6860 - accuracy: 0.3555 - val_loss: 1.9792 - val_accuracy: 0.5342 - 15s/epoch - 663ms/step\n",
      "Epoch 247/300\n",
      "22/22 - 15s - loss: 1.8605 - accuracy: 0.4870 - val_loss: 2.4421 - val_accuracy: 0.5616 - 15s/epoch - 664ms/step\n",
      "Epoch 248/300\n",
      "22/22 - 15s - loss: 2.3073 - accuracy: 0.4610 - val_loss: 2.4639 - val_accuracy: 0.4932 - 15s/epoch - 667ms/step\n",
      "Epoch 249/300\n",
      "22/22 - 15s - loss: 1.8937 - accuracy: 0.5188 - val_loss: 1.9439 - val_accuracy: 0.5479 - 15s/epoch - 662ms/step\n",
      "Epoch 250/300\n",
      "22/22 - 15s - loss: 1.6416 - accuracy: 0.5419 - val_loss: 2.0959 - val_accuracy: 0.5753 - 15s/epoch - 663ms/step\n",
      "Epoch 251/300\n",
      "22/22 - 14s - loss: 1.6919 - accuracy: 0.5043 - val_loss: 1.7309 - val_accuracy: 0.5479 - 14s/epoch - 658ms/step\n",
      "Epoch 252/300\n",
      "22/22 - 15s - loss: 2.2223 - accuracy: 0.4379 - val_loss: 2.4785 - val_accuracy: 0.5205 - 15s/epoch - 664ms/step\n",
      "Epoch 253/300\n",
      "22/22 - 15s - loss: 1.9193 - accuracy: 0.5130 - val_loss: 2.1722 - val_accuracy: 0.4521 - 15s/epoch - 663ms/step\n",
      "Epoch 254/300\n",
      "22/22 - 15s - loss: 1.7650 - accuracy: 0.4870 - val_loss: 2.3595 - val_accuracy: 0.5068 - 15s/epoch - 663ms/step\n",
      "Epoch 255/300\n",
      "22/22 - 15s - loss: 2.2031 - accuracy: 0.4566 - val_loss: 1.9295 - val_accuracy: 0.4932 - 15s/epoch - 666ms/step\n",
      "Epoch 256/300\n",
      "22/22 - 15s - loss: 1.7156 - accuracy: 0.5275 - val_loss: 2.1845 - val_accuracy: 0.5205 - 15s/epoch - 664ms/step\n",
      "Epoch 257/300\n",
      "22/22 - 15s - loss: 1.8540 - accuracy: 0.5014 - val_loss: 1.6483 - val_accuracy: 0.5890 - 15s/epoch - 665ms/step\n",
      "Epoch 258/300\n",
      "22/22 - 15s - loss: 1.8821 - accuracy: 0.5188 - val_loss: 2.1387 - val_accuracy: 0.5342 - 15s/epoch - 660ms/step\n",
      "Epoch 259/300\n",
      "22/22 - 15s - loss: 1.6057 - accuracy: 0.5506 - val_loss: 1.9446 - val_accuracy: 0.5753 - 15s/epoch - 660ms/step\n",
      "Epoch 260/300\n",
      "22/22 - 14s - loss: 2.0124 - accuracy: 0.4740 - val_loss: 2.0737 - val_accuracy: 0.4247 - 14s/epoch - 654ms/step\n",
      "Epoch 261/300\n",
      "22/22 - 15s - loss: 1.4708 - accuracy: 0.5780 - val_loss: 2.3892 - val_accuracy: 0.5068 - 15s/epoch - 665ms/step\n",
      "Epoch 262/300\n",
      "22/22 - 15s - loss: 1.6707 - accuracy: 0.5303 - val_loss: 1.7393 - val_accuracy: 0.5205 - 15s/epoch - 667ms/step\n",
      "Epoch 263/300\n",
      "22/22 - 16s - loss: 1.5949 - accuracy: 0.5332 - val_loss: 1.8876 - val_accuracy: 0.4932 - 16s/epoch - 731ms/step\n",
      "Epoch 264/300\n",
      "22/22 - 15s - loss: 1.4451 - accuracy: 0.5997 - val_loss: 2.0321 - val_accuracy: 0.5616 - 15s/epoch - 689ms/step\n",
      "Epoch 265/300\n",
      "22/22 - 15s - loss: 1.6497 - accuracy: 0.5087 - val_loss: 1.9934 - val_accuracy: 0.5753 - 15s/epoch - 702ms/step\n",
      "Epoch 266/300\n",
      "22/22 - 15s - loss: 1.8620 - accuracy: 0.5058 - val_loss: 2.0862 - val_accuracy: 0.4932 - 15s/epoch - 670ms/step\n",
      "Epoch 267/300\n",
      "22/22 - 15s - loss: 1.9242 - accuracy: 0.4870 - val_loss: 2.0368 - val_accuracy: 0.6164 - 15s/epoch - 663ms/step\n",
      "Epoch 268/300\n",
      "22/22 - 15s - loss: 1.7232 - accuracy: 0.4855 - val_loss: 1.9392 - val_accuracy: 0.5205 - 15s/epoch - 662ms/step\n",
      "Epoch 269/300\n",
      "22/22 - 15s - loss: 1.7852 - accuracy: 0.5231 - val_loss: 1.9167 - val_accuracy: 0.5068 - 15s/epoch - 662ms/step\n",
      "Epoch 270/300\n",
      "22/22 - 15s - loss: 1.4938 - accuracy: 0.5795 - val_loss: 2.4432 - val_accuracy: 0.5068 - 15s/epoch - 663ms/step\n",
      "Epoch 271/300\n",
      "22/22 - 15s - loss: 1.5786 - accuracy: 0.5419 - val_loss: 2.0481 - val_accuracy: 0.5342 - 15s/epoch - 662ms/step\n",
      "Epoch 272/300\n",
      "22/22 - 14s - loss: 1.9620 - accuracy: 0.4552 - val_loss: 2.3221 - val_accuracy: 0.4384 - 14s/epoch - 656ms/step\n",
      "Epoch 273/300\n",
      "22/22 - 15s - loss: 2.1716 - accuracy: 0.4725 - val_loss: 2.3432 - val_accuracy: 0.4658 - 15s/epoch - 677ms/step\n",
      "Epoch 274/300\n",
      "22/22 - 15s - loss: 1.9981 - accuracy: 0.4610 - val_loss: 2.0281 - val_accuracy: 0.5205 - 15s/epoch - 667ms/step\n",
      "Epoch 275/300\n",
      "22/22 - 15s - loss: 1.9290 - accuracy: 0.4986 - val_loss: 2.3969 - val_accuracy: 0.5205 - 15s/epoch - 667ms/step\n",
      "Epoch 276/300\n",
      "22/22 - 15s - loss: 1.6045 - accuracy: 0.5390 - val_loss: 1.9793 - val_accuracy: 0.4932 - 15s/epoch - 664ms/step\n",
      "Epoch 277/300\n",
      "22/22 - 15s - loss: 1.8679 - accuracy: 0.4769 - val_loss: 1.8343 - val_accuracy: 0.5616 - 15s/epoch - 667ms/step\n",
      "Epoch 278/300\n",
      "22/22 - 15s - loss: 1.6372 - accuracy: 0.5332 - val_loss: 1.7385 - val_accuracy: 0.5753 - 15s/epoch - 666ms/step\n",
      "Epoch 279/300\n",
      "22/22 - 14s - loss: 1.6327 - accuracy: 0.5419 - val_loss: 1.9806 - val_accuracy: 0.4658 - 14s/epoch - 656ms/step\n",
      "Epoch 280/300\n",
      "22/22 - 15s - loss: 1.7954 - accuracy: 0.5072 - val_loss: 2.6418 - val_accuracy: 0.5342 - 15s/epoch - 666ms/step\n",
      "Epoch 281/300\n",
      "22/22 - 15s - loss: 2.1860 - accuracy: 0.5000 - val_loss: 2.2020 - val_accuracy: 0.3973 - 15s/epoch - 663ms/step\n",
      "Epoch 282/300\n",
      "22/22 - 15s - loss: 1.8921 - accuracy: 0.5043 - val_loss: 2.3692 - val_accuracy: 0.5342 - 15s/epoch - 664ms/step\n",
      "Epoch 283/300\n",
      "22/22 - 15s - loss: 1.7146 - accuracy: 0.4986 - val_loss: 2.1208 - val_accuracy: 0.4384 - 15s/epoch - 663ms/step\n",
      "Epoch 284/300\n",
      "22/22 - 15s - loss: 1.7821 - accuracy: 0.5202 - val_loss: 2.6753 - val_accuracy: 0.5205 - 15s/epoch - 663ms/step\n",
      "Epoch 285/300\n",
      "22/22 - 15s - loss: 2.4898 - accuracy: 0.4321 - val_loss: 2.1288 - val_accuracy: 0.5205 - 15s/epoch - 668ms/step\n",
      "Epoch 286/300\n",
      "22/22 - 15s - loss: 2.1217 - accuracy: 0.4913 - val_loss: 2.6018 - val_accuracy: 0.5342 - 15s/epoch - 679ms/step\n",
      "Epoch 287/300\n",
      "22/22 - 15s - loss: 1.6291 - accuracy: 0.5564 - val_loss: 1.8497 - val_accuracy: 0.5479 - 15s/epoch - 680ms/step\n",
      "Epoch 288/300\n",
      "22/22 - 15s - loss: 1.3106 - accuracy: 0.6142 - val_loss: 2.1328 - val_accuracy: 0.4521 - 15s/epoch - 675ms/step\n",
      "Epoch 289/300\n",
      "22/22 - 15s - loss: 1.7850 - accuracy: 0.5101 - val_loss: 2.1634 - val_accuracy: 0.5205 - 15s/epoch - 668ms/step\n",
      "Epoch 290/300\n",
      "22/22 - 15s - loss: 1.5063 - accuracy: 0.5477 - val_loss: 1.7709 - val_accuracy: 0.5753 - 15s/epoch - 676ms/step\n",
      "Epoch 291/300\n",
      "22/22 - 15s - loss: 1.4466 - accuracy: 0.5477 - val_loss: 1.8931 - val_accuracy: 0.5753 - 15s/epoch - 682ms/step\n",
      "Epoch 292/300\n",
      "22/22 - 15s - loss: 1.8445 - accuracy: 0.4855 - val_loss: 1.9359 - val_accuracy: 0.5753 - 15s/epoch - 675ms/step\n",
      "Epoch 293/300\n",
      "22/22 - 15s - loss: 1.9022 - accuracy: 0.5159 - val_loss: 1.9948 - val_accuracy: 0.4658 - 15s/epoch - 664ms/step\n",
      "Epoch 294/300\n",
      "22/22 - 15s - loss: 1.7186 - accuracy: 0.5260 - val_loss: 2.5432 - val_accuracy: 0.4932 - 15s/epoch - 668ms/step\n",
      "Epoch 295/300\n",
      "22/22 - 15s - loss: 2.4005 - accuracy: 0.4104 - val_loss: 2.0957 - val_accuracy: 0.5205 - 15s/epoch - 670ms/step\n",
      "Epoch 296/300\n",
      "22/22 - 15s - loss: 1.8212 - accuracy: 0.5592 - val_loss: 2.2020 - val_accuracy: 0.4932 - 15s/epoch - 666ms/step\n",
      "Epoch 297/300\n",
      "22/22 - 15s - loss: 1.6285 - accuracy: 0.5491 - val_loss: 1.8880 - val_accuracy: 0.6301 - 15s/epoch - 666ms/step\n",
      "Epoch 298/300\n",
      "22/22 - 15s - loss: 2.0070 - accuracy: 0.4740 - val_loss: 2.3086 - val_accuracy: 0.5890 - 15s/epoch - 674ms/step\n",
      "Epoch 299/300\n",
      "22/22 - 15s - loss: 2.2695 - accuracy: 0.5043 - val_loss: 1.8854 - val_accuracy: 0.4658 - 15s/epoch - 669ms/step\n",
      "Epoch 300/300\n",
      "22/22 - 15s - loss: 1.8401 - accuracy: 0.4841 - val_loss: 1.6964 - val_accuracy: 0.5890 - 15s/epoch - 672ms/step\n"
     ]
    }
   ],
   "source": [
    "# model.compile(optimizer=Adam(learning_rate=0.006), loss='categorical_crossentropy', metrics=['accuracy']) #Last best setting\n",
    "# model.compile(optimizer=Adam(learning_rate=0.0045), loss='categorical_crossentropy', metrics=['accuracy']) #Lower LR\n",
    "model.compile(optimizer=Adam(learning_rate=0.0075), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True) #save best model\n",
    "\n",
    "#more epochs for better results\n",
    "result = model.fit(x=train, validation_data=validation, epochs=300, verbose=2, callbacks=[model_checkpoint_callback,cb])\n",
    "# result = model.fit(x=train, validation_data=validation, epochs=200, verbose=2, callbacks=[cb])\n",
    "# \n",
    "# The model weights (that are considered the best) are loaded into the\n",
    "# model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "plt.plot(result.history['accuracy'])\n",
    "plt.plot(result.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#loss\n",
    "plt.plot(result.history['loss'])\n",
    "plt.plot(result.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "result = model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate Test data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.054031503333334\n",
      "3/3 [==============================] - 2s 499ms/step - loss: 1.5969 - accuracy: 0.6067\n",
      "\n",
      "Test Loss: 1.596877932548523\n",
      "Test Accuracy: 0.6067415475845337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sum(cb.logs) / 150)\n",
    "\n",
    "testLoss, testAcc = model.evaluate(test)\n",
    "print('''\n",
    "Test Loss: {}\n",
    "Test Accuracy: {}\n",
    "'''.format(testLoss, testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>*Optional - Fine tune model with two-step training*</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "mobile.trainable = True\n",
    "\n",
    "for layer in mobile.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#re-train the model at a very low learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.00006), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "result = model.fit(x=train, validation_data=validation, epochs=100, verbose=2, callbacks=[cb, es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy and Loss graph</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "plt.plot(result.history['accuracy'])\n",
    "plt.plot(result.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#loss\n",
    "plt.plot(result.history['loss'])\n",
    "plt.plot(result.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification report</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test, verbose=0)\n",
    "# predictions = predictions.reshape(1,-1)[0]\n",
    "lst = np.argmax(predictions, axis= 1)\n",
    "\n",
    "print(classification_report(test.classes, lst, target_names = test.class_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Confusion matrix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = confusion_matrix(test.classes, lst)\n",
    "print(result)\n",
    "\n",
    "def plot_confusion_matrix(result, classes, cmap=plt.cm.Blues):\n",
    "    plt.subplots(figsize=(7, 7))\n",
    "    plt.imshow(result, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation = 90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "   \n",
    "\n",
    "    thresh = result.max() / 2.\n",
    "    for i, j in itertools.product(range(result.shape[0]), range(result.shape[1])):\n",
    "        plt.text(j, i, result[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color = \"white\" if result[i, j] > thresh else \"black\")\n",
    "        \n",
    "    #current matplotlib version will cut off top and bottom of the matrix, \n",
    "    #hence the manual workaround - remove if matplotlib version does not cut off\n",
    "    b, t = plt.ylim() # discover bottom and top values\n",
    "    b += 0.5 # Add 0.5 to bottom\n",
    "    t -= 0.5 # Subtract 0.5 from top\n",
    "    plt.ylim(b, t) # update ylim(bottom, top) values\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion_matrix(result, list(test.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Labels of wrong predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrors(test, predictions):\n",
    "    #get file names of all images in test data\n",
    "    names = test.filenames\n",
    "\n",
    "    #get IDs of all classes\n",
    "    allClassesID = test.classes\n",
    "\n",
    "    #get dictionary of classes and respective ID\n",
    "    classLabelDict = test.class_indices\n",
    "    \n",
    "    #list of the names of all classes\n",
    "    classLabels = list(classLabelDict.keys())\n",
    "\n",
    "    print(\"The list of classes: \", classLabels)\n",
    "\n",
    "    #get the most predicted class\n",
    "    predictedClasses = np.argmax(predictions, axis=1)\n",
    "\n",
    "    predictedErrors = np.where(predictedClasses != allClassesID)[0]\n",
    "\n",
    "    print(\"Number of errors = {}/{}\".format(len(predictedErrors),test.samples))\n",
    "\n",
    "    return classLabels, predictedErrors, names\n",
    "\n",
    "def showErrors(classLabels, predictedErrors, predictions, names):\n",
    "    # Show the errors\n",
    "    for i in range(len(errors)):\n",
    "        predictedClass = np.argmax(predictions[predictedErrors[i]])\n",
    "\n",
    "        predictedLabel = classLabels[predictedClass]\n",
    "\n",
    "        title = 'Original Picture : {}, Prediction : {}, Confidence/Percentage : {:.3f}'.format\\\n",
    "                (names[predictedErrors[i]].split('/')[0],predictedLabel, predictions[errors[i]][predictedClass])\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of class labels, prediction errors and file names\n",
    "labels, errors, names = getErrors(test, predictions)\n",
    "\n",
    "#show wrongly predicted images, prediction and and the percentage (confidence)\n",
    "showErrors(labels, errors, predictions, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the directory back to initial path (Image Classification folder)\n",
    "os.chdir(\"../..\")\n",
    "# os.listdir()\n",
    "\n",
    "#save the model so that it can be passed to edge device\n",
    "model.save(\"image_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
